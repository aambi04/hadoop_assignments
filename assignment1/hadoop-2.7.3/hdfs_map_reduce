Script started on Sat 14 Sep 2019 07:39:53 PM EDT
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[?1034h[aagarwal@hdserver hadoop-2.7.3]$ bin
bin/  bind  
[aagarwal@hdserver hadoop-2.7.3]$ bin
bin/  bind  
[aagarwal@hdserver hadoop-2.7.3]$ bin/hdfs dfs -mkdir /user
mkdir: `/user': File exists
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ bin/hdfs dfs -mkdir /user/aagarwal
mkdir: `/user/aagarwal': File exists
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ bin/hdfs dfs -mkdir /user/aagarwal[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kput etc/hadoop/ user/aagarwal/in put
put: `user/aagarwal/input': No such file or directory
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ bin/hdfs dfs -put etc/hadoop/ user/aagarwal/inpput[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C/user/aagarwal/input[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

put: `/user/aagarwal/input/hadoop/capacity-scheduler.xml': File exists
put: `/user/aagarwal/input/hadoop/configuration.xsl': File exists
put: `/user/aagarwal/input/hadoop/container-executor.cfg': File exists
put: `/user/aagarwal/input/hadoop/core-site.xml': File exists
put: `/user/aagarwal/input/hadoop/hadoop-env.cmd': File exists
put: `/user/aagarwal/input/hadoop/hadoop-env.sh': File exists
put: `/user/aagarwal/input/hadoop/hadoop-metrics.properties': File exists
put: `/user/aagarwal/input/hadoop/hadoop-metrics2.properties': File exists
put: `/user/aagarwal/input/hadoop/hadoop-policy.xml': File exists
put: `/user/aagarwal/input/hadoop/hdfs-site.xml': File exists
put: `/user/aagarwal/input/hadoop/httpfs-env.sh': File exists
put: `/user/aagarwal/input/hadoop/httpfs-log4j.properties': File exists
put: `/user/aagarwal/input/hadoop/httpfs-signature.secret': File exists
put: `/user/aagarwal/input/hadoop/httpfs-site.xml': File exists
put: `/user/aagarwal/input/hadoop/kms-acls.xml': File exists
put: `/user/aagarwal/input/hadoop/kms-env.sh': File exists
put: `/user/aagarwal/input/hadoop/kms-log4j.properties': File exists
put: `/user/aagarwal/input/hadoop/kms-site.xml': File exists
put: `/user/aagarwal/input/hadoop/log4j.properties': File exists
put: `/user/aagarwal/input/hadoop/mapred-env.cmd': File exists
put: `/user/aagarwal/input/hadoop/mapred-env.sh': File exists
put: `/user/aagarwal/input/hadoop/mapred-queues.xml.template': File exists
put: `/user/aagarwal/input/hadoop/mapred-site.xml.template': File exists
put: `/user/aagarwal/input/hadoop/slaves': File exists
put: `/user/aagarwal/input/hadoop/ssl-client.xml.example': File exists
put: `/user/aagarwal/input/hadoop/ssl-server.xml.example': File exists
put: `/user/aagarwal/input/hadoop/yarn-env.cmd': File exists
put: `/user/aagarwal/input/hadoop/yarn-env.sh': File exists
put: `/user/aagarwal/input/hadoop/yarn-site.xml': File exists
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$  bin/hadoop jar share/hadoop/mapreduce/hadoop- mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'
19/09/14 19:41:38 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/09/14 19:41:38 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/09/14 19:41:38 INFO input.FileInputFormat: Total input paths to process : 9
19/09/14 19:41:38 INFO mapreduce.JobSubmitter: number of splits:9
19/09/14 19:41:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1818283781_0001
19/09/14 19:41:39 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
19/09/14 19:41:39 INFO mapreduce.Job: Running job: job_local1818283781_0001
19/09/14 19:41:39 INFO mapred.LocalJobRunner: OutputCommitter set in config null
19/09/14 19:41:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:39 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/09/14 19:41:39 INFO mapred.LocalJobRunner: Waiting for map tasks
19/09/14 19:41:39 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000000_0
19/09/14 19:41:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:39 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/hadoop-policy.xml:0+9683
19/09/14 19:41:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:39 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:39 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:39 INFO mapred.LocalJobRunner: 
19/09/14 19:41:39 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:39 INFO mapred.MapTask: Spilling map output
19/09/14 19:41:39 INFO mapred.MapTask: bufstart = 0; bufend = 17; bufvoid = 104857600
19/09/14 19:41:39 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600
19/09/14 19:41:39 INFO mapred.MapTask: Finished spill 0
19/09/14 19:41:39 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000000_0 is done. And is in the process of committing
19/09/14 19:41:39 INFO mapred.LocalJobRunner: map
19/09/14 19:41:39 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000000_0' done.
19/09/14 19:41:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000000_0
19/09/14 19:41:39 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000001_0
19/09/14 19:41:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:39 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/kms-site.xml:0+5511
19/09/14 19:41:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:39 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:39 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:39 INFO mapred.LocalJobRunner: 
19/09/14 19:41:39 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000001_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000001_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000001_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000002_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/capacity-scheduler.xml:0+4436
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000002_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000002_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000002_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000003_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/kms-acls.xml:0+3518
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000003_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000003_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000003_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000004_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/hdfs-site.xml:0+775
19/09/14 19:41:40 INFO mapreduce.Job: Job job_local1818283781_0001 running in uber mode : false
19/09/14 19:41:40 INFO mapreduce.Job:  map 100% reduce 0%
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000004_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000004_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000004_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000005_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/core-site.xml:0+774
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000005_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000005_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000005_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000006_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/yarn-site.xml:0+690
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000006_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000006_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000006_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000007_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/httpfs-site.xml:0+620
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.LocalJobRunner: 
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.Task: Task:attempt_local1818283781_0001_m_000007_0 is done. And is in the process of committing
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map
19/09/14 19:41:40 INFO mapred.Task: Task 'attempt_local1818283781_0001_m_000007_0' done.
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818283781_0001_m_000007_0
19/09/14 19:41:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1818283781_0001_m_000008_0
19/09/14 19:41:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/09/14 19:41:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
19/09/14 19:41:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/aagarwal/input/hadoop:0+0
19/09/14 19:41:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
19/09/14 19:41:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
19/09/14 19:41:40 INFO mapred.MapTask: soft limit at 83886080
19/09/14 19:41:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
19/09/14 19:41:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
19/09/14 19:41:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
19/09/14 19:41:40 INFO mapred.MapTask: Starting flush of map output
19/09/14 19:41:40 INFO mapred.LocalJobRunner: map task executor complete.
19/09/14 19:41:41 WARN mapred.LocalJobRunner: job_local1818283781_0001
java.lang.Exception: java.io.FileNotFoundException: Path is not a file: /user/aagarwal/input/hadoop
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:588)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.FileNotFoundException: Path is not a file: /user/aagarwal/input/hadoop
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:588)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:548)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:786)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /user/aagarwal/input/hadoop
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:588)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)
	... 21 more
19/09/14 19:41:41 INFO mapreduce.Job: Job job_local1818283781_0001 failed with state FAILED due to: NA
19/09/14 19:41:41 INFO mapreduce.Job: Counters: 23
	File System Counters
		FILE: Number of bytes read=2404700
		FILE: Number of bytes written=4705264
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=167669
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=96
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Map-Reduce Framework
		Map input records=745
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=67
		Input split bytes=973
		Combine input records=1
		Combine output records=1
		Spilled Records=1
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=227
		Total committed heap usage (bytes)=1243992064
	File Input Format Counters 
		Bytes Read=26007
19/09/14 19:41:41 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/aagarwal/output already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at org.apache.hadoop.examples.Grep.run(Grep.java:94)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.examples.Grep.main(Grep.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ bin/hdfs dfs -get ouptut [K[K[K[K[Ktput output
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ cat output/*
1	dfsadmin
]0;aagarwal@hdserver:~/Documents/all_assignments/assignment1/hadoop-2.7.3[aagarwal@hdserver hadoop-2.7.3]$ exit
exit

Script done on Sat 14 Sep 2019 07:43:15 PM EDT
